<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://zmccormick7.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zmccormick7.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-27T17:54:17+00:00</updated><id>https://zmccormick7.github.io/feed.xml</id><title type="html">blank</title><subtitle>My perspective on LLM, RAG, and future of AI. </subtitle><entry><title type="html">Why chat and RAG don’t mix</title><link href="https://zmccormick7.github.io/blog/2025/rag-chat/" rel="alternate" type="text/html" title="Why chat and RAG don’t mix"/><published>2025-03-26T16:40:16+00:00</published><updated>2025-03-26T16:40:16+00:00</updated><id>https://zmccormick7.github.io/blog/2025/rag-chat</id><content type="html" xml:base="https://zmccormick7.github.io/blog/2025/rag-chat/"><![CDATA[<p>Chat interfaces have become the default for LLM interactions due to their simplicity and familiarity. Users type natural language queries and receive natural language responses without learning specialized commands or workflows. ChatGPT has trained us well.</p> <p>However, this simplicity creates a technical challenge for AI application builders who want to add document interaction capabilities to their apps: while users approach chat expecting comprehensive document interaction capabilities, standard RAG implementations are narrowly optimized for factoid question answering. They fail on a wide variety of seemingly simple tasks. Let’s discuss why.</p> <h3 id="technical-limitations-of-standard-rag">Technical Limitations of Standard RAG</h3> <p>Traditional RAG architectures excel at extracting discrete pieces of information from documents: “What was our Q3 revenue?” or “When did we change our refund policy?” They operate by chunking documents, embedding those chunks, retrieving relevant sections via vector similarity, and providing these as context to the LLM. This works well when there is an answer to be found directly in the documents, and when that answer is contained in a relatively small pieces of text.</p> <p>However, users frequently go far beyond the narrow scope of tasks RAG systems are designed to handle. Here are a few examples of common document tasks that standard RAG isn’t designed to handle:</p> <ul> <li>Document summarization <ul> <li>This is an extremely simple sounding use case, and it’s the go-to “test prompt” for many users when they first try a new AI app that allows for uploading their docs. But it generally won’t work with a pure RAG approach.</li> </ul> </li> <li>Comparative analyses (necessitating structured comparison between multiple documents) <ul> <li>This will work well with certain types of queries for some RAG implementations, but not all.</li> </ul> </li> <li>Pattern identification (requiring analysis across document collections) <ul> <li>“How have analyst opinions about NVDA changed over the last four quarters?”</li> </ul> </li> <li>Multi-step reasoning tasks <ul> <li>“Read this policy document to understand the qualification requirements and then read this other document with my information to figure out if I qualify.”</li> </ul> </li> </ul> <p>These operations fundamentally exceed what retrieval-based architectures were designed to handle. The core issue isn’t implementation quality but rather architectural limitations: standard RAG was never engineered to support the full spectrum of document-centric operations users attempt to perform. Using a chat interface exacerbates this problem because users have been conditioned to view chat as a do-it-all UI. Why wouldn’t “chat, but with my docs” be able to do everything chat can do, but with my docs as context?</p> <p>This architectural mismatch leads to inconsistent system performance. When users request operations that align with the retrieval paradigm, the system performs well. When they request operations requiring document-level understanding or complex transformations, the same system falters. The result is unpredictable behavior from the user perspective, as the system’s capabilities don’t align with the affordances suggested by the chat interface.</p> <h3 id="implementation-alternatives">Implementation Alternatives</h3> <p>Two technical approaches can address this fundamental architecture-expectation gap:</p> <p><strong>1. Interface Specialization</strong></p> <p>This approach replaces the general chat interface with purpose-built UI flows for specific document operations. Each specialized interface can implement the optimal technical approach for its specific function:</p> <ul> <li>Dedicated summarization modules with document-level context management</li> <li>Structured comparison views with explicit document alignment algorithms</li> <li>Transformation interfaces with specialized prompt engineering</li> </ul> <p>This creates a more predictable user experience by making system capabilities explicit. Your users are much less likely to try to do something the system isn’t designed to handle. I like to think of this approach as “putting the UX on rails.”</p> <p><strong>2. Agent-Based Architecture</strong></p> <p>Alternatively, a more sophisticated agent-based architecture can maintain the chat interface while expanding its capabilities. In this model:</p> <ul> <li>RAG becomes one tool out of many</li> <li>The agent analyzes user requests to determine required operations</li> <li>Specialized modules are invoked based on operation type</li> <li>Results are composed into coherent responses</li> </ul> <p>This approach preserves interface simplicity while implementing a more robust technical architecture behind the scenes. It requires more complex orchestration logic but delivers a more consistent user experience. This approach has its drawbacks, though. First, it takes more engineering effort upfront to get a reliable product up and running. Second, it’s generally slower and more expensive, which has both UX and financial impacts. And finally, it’s never going to be as reliable as a more specialized interface that only does one or two things.</p> <h3 id="which-approach-should-you-pick">Which approach should you pick?</h3> <p>Your first choice should always be interface specialization. If you’re building an AI application and its name isn’t ChatGPT, then your whole reason for existing is to build something more specialized than ChatGPT. You’re not going to beat OpenAI at their own game. You might be specializing horizontally (marketing automation platform) or vertically (AI for financial advisors). Either way, you should be able to come up with user flows that are more purpose-built than “type into this chat box and get a response.”</p> <p>Now you may still want to layer on agentic capabilities within a more specialized UI. But that should be Step 2, not Step 1.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Chat interfaces have become the default for LLM interactions due to their simplicity and familiarity. Users type natural language queries and receive natural language responses without learning specialized commands or workflows. ChatGPT has trained us well.]]></summary></entry><entry><title type="html">Contextual vs. non-contextual LLM tasks</title><link href="https://zmccormick7.github.io/blog/2025/contextual-tasks/" rel="alternate" type="text/html" title="Contextual vs. non-contextual LLM tasks"/><published>2025-03-03T16:40:16+00:00</published><updated>2025-03-03T16:40:16+00:00</updated><id>https://zmccormick7.github.io/blog/2025/contextual-tasks</id><content type="html" xml:base="https://zmccormick7.github.io/blog/2025/contextual-tasks/"><![CDATA[<p>I like to think of LLM applications as falling into one of two categories: contextual tasks and non-contextual tasks. Contextual tasks require knowledge about the user or organization to complete. Non-contextual tasks do not. Let’s consider some examples of each:</p> <p>Contextual tasks</p> <ul> <li>Email triage and drafting <ul> <li>Requires deep context about the user to know what’s relevant and what’s not.</li> </ul> </li> <li>Evaluating resumes and interview performance <ul> <li>Requires context about the company and the role.</li> </ul> </li> <li>Marketing content generation <ul> <li>Requires context about the company’s brand, marketing strategy, past campaigns, etc.</li> </ul> </li> <li>Customer support <ul> <li>Requires context about the company and its products, and ideally context about the customer too.</li> </ul> </li> </ul> <p>Non-contextual tasks</p> <ul> <li>Document summarization <ul> <li>You don’t need to know anything about the user to properly summarize a document for them. You just need the document.</li> </ul> </li> <li>Most structured data extraction tasks <ul> <li>Looping through a document and extracting specific types of data usually doesn’t require any additional context about the user. There are exceptions, though.</li> </ul> </li> <li>Translation</li> <li>Spam filtering <ul> <li>Spam is spam</li> </ul> </li> </ul> <p>If you treat a contextual task as if it’s a non-contextual task, you’ll get disappointing results. Consider the marketing content generation task. If all you provide the LLM with is a short description of a product, and you ask it to generate marketing copy for it, you’ll get a very bland and generic response. It won’t be tailored to the company’s brand, and it won’t feel consistent with all of the other marketing copy the company uses.</p> <h2 id="not-all-contextual-tasks-are-rag-tasks">Not all contextual tasks are RAG tasks</h2> <p>Ok so let’s say you have a contextual task. At this point, most people jump straight to RAG. That’s a mistake.</p> <p>For many contextual tasks, the context the LLM needs can be provided via a fixed context string, rather than being constructed through a retrieval process for each user input. It only becomes a RAG task when the context required is highly dependent on the user input. Customer support is usually a RAG task, for example, because customers can have a wide range of issues they need help with, and responding to each one requires detailed information about specific company policies and products. You wouldn’t want to put all of that information into a single fixed prompt, because 99% of it would be irrelevant for a given user input.</p> <p>What do you do if your contextual task isn’t a RAG task?</p> <p>If your task needs context but doesn’t require retrieval for each input, fixed context prompts are your best bet. Think about what the LLM actually needs to know to do a good job. For marketing tasks, this might be brand guidelines, tone of voice, and examples of past successful content. For email drafting, it could be your communication style and common response patterns. You need to include enough context for the LLM to understand what makes your company unique without bloating your prompts with unnecessary information. This generally requires a bit of trial and error.</p> <p>With the ever-expanding context windows of modern LLMs, the fixed context approach is becoming viable for more and more use cases over time. A good rule of thumb (that will surely be outdated in six months) is that if you can pack everything the LLM needs to know into 10k tokens (~15 pages) then you should use fixed context. It’ll be more reliable than RAG, because the context will always be there. With RAG, you’re adding one additional point of failure (the search system).</p> <p>Even for RAG tasks, there’s usually some part of the context that should be provided as fixed context. For example, in the customer support use case, there will be certain high-level information that the LLM always needs. For example, a detailed description of the company and an overview of its products. This information is helpful to the LLM because it provides context to user queries and RAG search results, reducing the risk of hallucinations. Commonly requested information, like return policies, should also be provided via fixed context, to minimize the risk of a retrieval failure leading to a bad response.</p> <p>So to sum it up: First, do you have a contextual task or not? If not, you have it easy. If you do have a contextual task, first add as much information as you can to a fixed context string. Then, if it doesn’t all fit, consider RAG.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I like to think of LLM applications as falling into one of two categories: contextual tasks and non-contextual tasks. Contextual tasks require knowledge about the user or organization to complete. Non-contextual tasks do not. Let’s consider some examples of each:]]></summary></entry></feed>